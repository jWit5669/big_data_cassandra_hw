# -*- coding: utf-8 -*-
"""big data cassandra hw.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nKcXbpF2cus45CSJMPWalxkPYYWaslH5
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade astrapy

# Commented out IPython magic to ensure Python compatibility.
# %pip install cassandra-driver

from astrapy import DataAPIClient
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
import json
import pandas as pd

with open( "/content/drive/MyDrive/PhD/Big Data Tools & Techniques/big_data_db-token.json" ) as f:
  secrets = json.load( f )

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]
TOKEN = secrets["token"]

# This secure connect bundle is autogenerated when you download your SCB,
# if yours is different update the file name below
cloud_config= {
  'secure_connect_bundle': '/content/drive/MyDrive/PhD/Big Data Tools & Techniques/secure-connect-big-data-db.zip'
}

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]

auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
session = cluster.connect()

# Using code provided by DataStax Astra for this whole chunk
row = session.execute("select release_version from system.local").one()
if row:
  print(row[0])
else:
  print("An error occurred.")

# Don't usually hardcode schema/table names, but I did here
session.execute("USE sales")

# I was having an issue pulling the data regularly, so Gemini suggested I use the raw data using this variant of the link to parse it
url = 'https://raw.githubusercontent.com/gchandra10/filestorage/main/sales_100.csv'
df = pd.read_csv(url,parse_dates=[0])

# Cassandra requires an ID and realistically I could have used Order ID, but if I ever pull from a different csv, this is better since I won't have to find that csv's id
df.insert(0, 'id', [int(x) for x in range(len(df))])

# Didn't want to hardcode a string, so I used if-else to map python datatypes to Cassandra datatypes and made a long string that I can execute
# id is hardcoded since the syntax for primary key is different

def create_cql_table_string(df, keyspace, table_name):
    cql = f"CREATE TABLE IF NOT EXISTS {keyspace}.{table_name} (\n"
    cql += f"    id int PRIMARY KEY,\n"
    for col in df.drop(['id'], axis=1).columns:
        dtype = df[col].dtype
        # Enclose column names in double quotes if they contain spaces
        col_name = f"\"{col}\"" if ' ' in col else col
        if dtype == 'int64':
            cql += f"    {col_name} int,\n"
        elif dtype == 'float64':
            cql += f"    {col_name} double,\n"
        elif dtype == 'object':
            cql += f"    {col_name} text,\n"
        elif dtype == '<M8[ns]':
            cql += f"    {col_name} timestamp,\n"
        else:
            cql += f"    {col_name} text,\n"

    cql = cql[:-2] + "\n"  # Remove trailing comma and add a new line
    cql += ")"
    return cql

session.execute( create_cql_table_string(df, "sales", "sales_table") )

# Wanted to drop duplicates before populating the table. Did this using the Order ID since I know for a fact that the id I made is unique
df = df.drop_duplicates(subset=['Order ID'], keep='first')

def populate_table( df, keyspace, table_name ):

  # Don't want to throw duplicate data into our table, so I have a try-catch that only populates if the table is empty or doesn't exist
  rows = session.execute(f"SELECT * FROM {keyspace}.{table_name}")

  try:
    rows[0]
  except IndexError:

    # Dynamic INSERT INTO using all columns and is then executed and rows are inserted one by one with the loop in the bottom of this function
    columns_quoted = [f'"{col}"' if ' ' in col or col in ['type'] else col for col in df.columns]
    insert_query = f'INSERT INTO {keyspace}.{table_name} ( ' + ', '.join(columns_quoted) + ')' + ' VALUES ( ' + ', '.join(['?']*len(df.columns)) + ' )'

    cql_query = session.prepare( insert_query )
    for index, row in df.iterrows():
      session.execute( cql_query, list(row) )

# Wasn't sure what to do for my gold layer, so I did 3 aggregate tables. Wasn't sure how to write the queries in CQL, so I transformed the data using pandas

def gold_1():

  # Gold 1 is showing the Number of Orders, Total Units Sold, Total Revenue, Total Profit, Profit per Order, and Profit per unit for each region in the data
  gold1 = df.groupby('Region').agg(
    Orders=('Order ID', 'count'),
    Total_Units=('UnitsSold', 'sum'),
    Total_Revenue=('TotalRevenue', 'sum'),
    Total_Profit=('TotalProfit', 'sum') ).sort_values(by='Orders', ascending=True).reset_index()
  gold1['Profit Per Order'] = gold1['Total_Profit'] / gold1['Orders']
  gold1['Profit Per Unit'] = gold1['Total_Profit'] / gold1['Total_Units']
  gold1.insert(0, 'id', [int(x) for x in range(len(gold1))])

  session.execute( create_cql_table_string(gold1, "sales", "gold1") )
  populate_table( gold1, 'sales', 'gold1' )

def gold_2():

  # Gold 2 is showing the chief export of each region. This, the way I approached it, is the item type that each region exports the most of
  # For this I aggregated at the level of region, summed units sold, used that to sort descendingly and only kept the first appearance for each region
  gold2 = df.groupby(['Region', 'Item Type']).agg(
    Orders=('Order ID', 'count'),
    Total_Units=('UnitsSold', 'sum'),
    Total_Revenue=('TotalRevenue', 'sum'),
    Total_Profit=('TotalProfit', 'sum') ).sort_values(by='Total_Units', ascending=False).reset_index().drop_duplicates(subset=['Region'], keep='first')
  gold2.insert(0, 'id', [int(x) for x in range(len(gold2))])

  session.execute( create_cql_table_string(gold2, "sales", "gold2") )
  populate_table( gold2, 'sales', 'gold2' )

def gold_3():

  # Gold 3 is the number of orders for each year of sales
  gold3 = df.groupby(pd.to_datetime(df['Order Date']).dt.year.astype('str') ).agg(
      Orders=('Order ID', 'count'),
      Total_Units=('UnitsSold', 'sum'),
      Total_Revenue=('TotalRevenue', 'sum'),
      Total_Profit=('TotalProfit', 'sum') ).sort_values(by='Order Date', ascending=True).reset_index()
  gold3.insert(0, 'id', [int(x) for x in range(len(gold3))])

  session.execute( create_cql_table_string(gold3, "sales", "gold3") )
  populate_table( gold3, 'sales', 'gold3' )

gold_1()
gold_2()
gold_3()